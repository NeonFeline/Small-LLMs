{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:36:37.817439Z",
     "start_time": "2025-10-21T22:36:33.516578Z"
    }
   },
   "source": "import torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch.nn.functional as F\n\nimport wandb\nimport tiktoken\nfrom tiktoken import Encoding\nfrom Transformer import TransformerDecoderWithRoPE\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai164201/projects/smallm/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ai164201/projects/smallm/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:36:37.959551Z",
     "start_time": "2025-10-21T22:36:37.949612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "class AsciiTokenizer():\n",
    "    def __init__(self, vocab_size=128):\n",
    "        self.vocab_size = vocab_size  # ASCII range\n",
    "        self.pad_token = 0  # Padding token index\n",
    "        self.unk_token = 1  # Unknown token index\n",
    "\n",
    "    def encode(self, text, seq_length):\n",
    "        tokens = [ord(c) if ord(c) < self.vocab_size else self.unk_token for c in text]\n",
    "        if len(tokens) < seq_length:\n",
    "            tokens += [self.pad_token] * (seq_length - len(tokens))  # Pad\n",
    "        else:\n",
    "            tokens = tokens[:seq_length]  # Truncate\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        chars = [chr(t) if t < self.vocab_size else '?' for t in tokens if t != self.pad_token]\n",
    "        return ''.join(chars)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset_name = \"wikipedia\", tokenizer=None, seq_length=64):\n",
    "        self.tokenizer = tokenizer if tokenizer else AsciiTokenizer()\n",
    "        self.seq_length = seq_length\n",
    "        if dataset_name == \"wikipedia\":\n",
    "            from datasets import load_dataset\n",
    "            self.dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")['train']\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_pad_token(self):\n",
    "        if isinstance(self.tokenizer, AsciiTokenizer):\n",
    "            return self.tokenizer.pad_token\n",
    "        elif isinstance(self.tokenizer, Encoding):\n",
    "            return self.tokenizer.special_tokens[\"<|pad|>\"]\n",
    "        else:\n",
    "            raise ValueError(\"Tokenizer must be either AsciiTokenizer or tiktoken.Encoding\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check if using ascii tokenizer\n",
    "        if isinstance(self.tokenizer, AsciiTokenizer):\n",
    "            # select only seq_length + 1 tokens for input-target pair\n",
    "\n",
    "            text = self.dataset[idx]\n",
    "            # random int between 0 and len(text) - seq_length - 1\n",
    "            if len(text['text']) < self.seq_length + 1:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                upper_bound = len(text['text']) - self.seq_length - 1\n",
    "                if upper_bound <= 0:\n",
    "                    start_idx = 0\n",
    "                else:\n",
    "                    start_idx = torch.randint(0, upper_bound, (1,)).item()\n",
    "            text['text'] = text['text'][start_idx:start_idx + self.seq_length + 1]\n",
    "\n",
    "            code = self.tokenizer.encode(text['text'], self.seq_length)\n",
    "            tokens = torch.tensor(code, dtype=torch.long)\n",
    "\n",
    "            # pad if needed\n",
    "            if len(tokens) < self.seq_length + 1:\n",
    "                padding = torch.full((self.seq_length + 1 - len(tokens),), self.tokenizer.pad_token, dtype=torch.long)\n",
    "                tokens = torch.cat([tokens, padding], dim=0)\n",
    "        elif isinstance(self.tokenizer, Encoding):\n",
    "            text = self.dataset[idx]\n",
    "            tokens = enc.encode(text)\n",
    "            tokens = tokens[:self.seq_length + 1]  # Truncate\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "            # pad if needed\n",
    "            if len(tokens) < self.seq_length + 1:\n",
    "                padding = torch.full((self.seq_length + 1 - len(tokens),), 0, dtype=torch.long)\n",
    "                tokens = torch.cat([tokens, padding], dim=0)\n",
    "        else:\n",
    "            raise ValueError(\"Tokenizer must be either AsciiTokenizer or tiktoken.Encoding\")\n",
    "            \n",
    "\n",
    "        return {\"input_ids\": tokens[:-1], \"target_ids\": tokens[1:]}  # Shifted for language modeling"
   ],
   "id": "e57a1d36ca31759c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:36:37.982597Z",
     "start_time": "2025-10-21T22:36:37.980449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "# enc = tiktoken.Encoding(\n",
    "#     name=enc.name,\n",
    "#     pat_str=enc._pat_str,\n",
    "#     mergeable_ranks=enc._mergeable_ranks,\n",
    "#     special_tokens={**enc._special_tokens, \"<|pad|>\": len(enc._mergeable_ranks)}\n",
    "# )\n",
    "#\n",
    "# enc._special_tokens"
   ],
   "id": "3acf7289fbaede4c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:36:38.043853Z",
     "start_time": "2025-10-21T22:36:38.041894Z"
    }
   },
   "cell_type": "code",
   "source": "# enc.encode(\"Hello, world!\")",
   "id": "45957640d4d0c338",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:36:42.679536Z",
     "start_time": "2025-10-21T22:36:38.104551Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = TextDataset(seq_length = 128)",
   "id": "41012a47146290ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "335caf1afe444fad98bb69b1c52c8d8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7621a308ff9147c1b6a821b5e3775b7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "trusted": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-21T22:36:42.778529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test both implementations\n",
    "input_size = 10\n",
    "hidden_size = 768\n",
    "batch_size = 256 + 128\n",
    "seq_len = 512\n",
    "learning_rate = 0.0001\n",
    "num_layers = 16\n",
    "epochs = 50\n",
    "max_grad_norm = 1.0  # Gradient clipping value\n",
    "vocab_size = 128\n",
    "warmup_steps = 2000\n",
    "file_name = \"transformerLargeContext\"\n",
    "checkpoint_dir = f\"{file_name}/checkpoints\"\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    project=\"smaLLM\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"Transformer decoder with RoPE\",\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"dropout\": 0.1,\n",
    "        \"layer_size\": hidden_size,\n",
    "        \"seq_length\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dataset\": \"Wikipedia english\",\n",
    "        \"epochs\": epochs,\n",
    "        \"grad_clipping\": max_grad_norm,\n",
    "        \"mixed_precision\": True,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"LambdaLR with warmup\",\n",
    "        \"tokenizer\": \"AsciiTokenizer (128 vocab size)\",\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, scaler, epoch, batch, loss, checkpoint_path):\n",
    "    \"\"\"Save complete training state\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'batch': batch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler, device):\n",
    "    \"\"\"Load complete training state\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    batch = checkpoint['batch']\n",
    "\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}, batch {batch}\")\n",
    "    return epoch, batch\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the most recent checkpoint\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    # Sort by modification time\n",
    "    latest = max(checkpoints, key=lambda f: os.path.getmtime(os.path.join(checkpoint_dir, f)))\n",
    "    return os.path.join(checkpoint_dir, latest)\n",
    "\n",
    "print(\"Testing Sequential version:\")\n",
    "model1 = TransformerDecoderWithRoPE(vocab_size=vocab_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "pad_token = dataset.get_pad_token()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: min((step+1)/warmup_steps, 1))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Try to load checkpoint\n",
    "start_epoch = 0\n",
    "start_batch = 0\n",
    "latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    start_epoch, start_batch = load_checkpoint(latest_checkpoint, model1, optimizer, scheduler, scaler, device)\n",
    "    start_epoch_offset = start_epoch  # Resume from this epoch\n",
    "else:\n",
    "    start_epoch_offset = 0\n",
    "\n",
    "for epoch in range(start_epoch_offset, epochs):\n",
    "    # If resuming mid-epoch, start from where we left off\n",
    "    batch_start = start_batch if epoch == start_epoch_offset else 0\n",
    "\n",
    "    for k, batch in enumerate(dataloader):\n",
    "        if k < batch_start:\n",
    "            continue\n",
    "\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = batch['target_ids'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision context\n",
    "        with autocast():\n",
    "            outputs = model1(inputs)\n",
    "            loss = criterion(outputs.view(-1, 128), targets.view(-1))\n",
    "\n",
    "        # Scale gradients, backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "\n",
    "        # Optimizer step and scaler update\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        run.log({\"train/loss\": loss.item(),\n",
    "                  \"epoch\": epoch,\n",
    "                  \"batch\": k,\n",
    "                  \"lr\": scheduler.get_last_lr()[0]\n",
    "                 })\n",
    "\n",
    "        if k % 1000 == 0 and k > 0:\n",
    "            checkpoint_path = f\"{checkpoint_dir}/{file_name}_epoch{epoch}_batch{k}.pt\"\n",
    "            save_checkpoint(model1, optimizer, scheduler, scaler, epoch, k, loss.item(), checkpoint_path)"
   ],
   "id": "955b462dc998f6af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmaciej-lachut\u001B[0m (\u001B[33mmaciej-lachut-poznan-university-of-technology\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ai164201/projects/smallm/wandb/run-20251022_003644-ytxhalob</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM/runs/ytxhalob' target=\"_blank\">peachy-eon-22</a></strong> to <a href='https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM' target=\"_blank\">https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM/runs/ytxhalob' target=\"_blank\">https://wandb.ai/maciej-lachut-poznan-university-of-technology/smaLLM/runs/ytxhalob</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Sequential version:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai164201/projects/smallm/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_65108/3283609979.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/home/ai164201/projects/smallm/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_65108/3283609979.py:120: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false,
    "ExecuteTime": {
     "end_time": "2025-10-21T22:34:46.716160379Z",
     "start_time": "2025-10-21T22:33:48.335696Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "836b6bc3f3ab1c40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 199999, '<|endofprompt|>': 200018, '<|pad|>': 199998}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
