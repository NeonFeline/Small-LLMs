{
 "cells": [
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n"
   ],
   "id": "a0b09a722713004e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "class AsciiTokenizer():\n",
    "    def __init__(self, vocab_size=128):\n",
    "        self.vocab_size = vocab_size  # ASCII range\n",
    "        self.pad_token = 0  # Padding token index\n",
    "        self.unk_token = 1  # Unknown token index\n",
    "\n",
    "    def encode(self, text, seq_length):\n",
    "        tokens = [ord(c) if ord(c) < self.vocab_size else self.unk_token for c in text]\n",
    "        if len(tokens) < seq_length:\n",
    "            tokens += [self.pad_token] * (seq_length - len(tokens))  # Pad\n",
    "        else:\n",
    "            tokens = tokens[:seq_length]  # Truncate\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        chars = [chr(t) if t < self.vocab_size else '?' for t in tokens if t != self.pad_token]\n",
    "        return ''.join(chars)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset_name = \"wikipedia\", tokeknizer=None, seq_length=64):\n",
    "        self.tokenizer = tokeknizer if tokeknizer else AsciiTokenizer()\n",
    "        self.seq_length = seq_length\n",
    "        if dataset_name == \"wikipedia\":\n",
    "            from datasets import load_dataset\n",
    "            self.dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")['train']\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset {dataset_name} not supported.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check if using ascii tokenizer\n",
    "        if isinstance(self.tokenizer, AsciiTokenizer):\n",
    "            # select only seq_length + 1 tokens for input-target pair\n",
    "\n",
    "            text = self.dataset[idx]\n",
    "            # random int between 0 and len(text) - seq_length - 1\n",
    "            if len(text['text']) < self.seq_length + 1:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                upper_bound = len(text['text']) - self.seq_length - 1\n",
    "                if upper_bound <= 0:\n",
    "                    start_idx = 0\n",
    "                else:\n",
    "                    start_idx = torch.randint(0, upper_bound, (1,)).item()\n",
    "            text['text'] = text['text'][start_idx:start_idx + self.seq_length + 1]\n",
    "\n",
    "            code = self.tokenizer.encode(text['text'], self.seq_length)\n",
    "            tokens = torch.tensor(code, dtype=torch.long)\n",
    "\n",
    "            # pad if needed\n",
    "            if len(tokens) < self.seq_length + 1:\n",
    "                padding = torch.full((self.seq_length + 1 - len(tokens),), self.tokenizer.pad_token, dtype=torch.long)\n",
    "                tokens = torch.cat([tokens, padding], dim=0)\n",
    "        else:\n",
    "            raise ValueError(\"Only AsciiTokenizer is supported now.\")\n",
    "\n",
    "       \n",
    "        return {\"input_ids\": tokens[:-1], \"target_ids\": tokens[1:]}  # Shifted for language modeling"
   ],
   "id": "eea689c424b5920f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = TextDataset(seq_length = 128)",
   "id": "ae620cc9ea3c450c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.tokenizer.decode(dataset[0]['target_ids'].tolist())",
   "id": "510576e68517c70b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResidualRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with residual connection that adds input projection to RNN output.\n",
    "\n",
    "    Args:\n",
    "        input_size: Size of input features\n",
    "        hidden_size: Size of RNN hidden state\n",
    "        output_size: Size of output features\n",
    "        num_layers: Number of RNN layers (default: 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(ResidualRNN, self).__init__()\n",
    "\n",
    "        # Validate inputs\n",
    "        if input_size <= 0 or hidden_size <= 0 or output_size <= 0:\n",
    "            raise ValueError(\"All sizes must be positive integers\")\n",
    "        if num_layers <= 0:\n",
    "            raise ValueError(\"num_layers must be positive\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Project input to hidden_size for residual connection\n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, last_hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass with residual connection.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "            last_hidden: Previous hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "                        If None, initializes to zeros\n",
    "\n",
    "        Returns:\n",
    "            out: Output tensor of shape (batch_size, seq_len, output_size)\n",
    "            hidden: Final hidden state of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Validate input shape\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D input (batch, seq, features), got {x.dim()}D\")\n",
    "\n",
    "        batch_size, seq_len, feat_size = x.shape\n",
    "\n",
    "        if feat_size != self.input_size:\n",
    "            raise ValueError(\n",
    "                f\"Input feature size {feat_size} doesn't match expected {self.input_size}\"\n",
    "            )\n",
    "\n",
    "        # Initialize hidden state if not provided\n",
    "        if last_hidden is None:\n",
    "            last_hidden = torch.zeros(\n",
    "                self.num_layers, batch_size, self.hidden_size,\n",
    "                device=x.device, dtype=x.dtype\n",
    "            )\n",
    "        else:\n",
    "            # Validate hidden state shape\n",
    "            if last_hidden.shape != (self.num_layers, batch_size, self.hidden_size):\n",
    "                raise ValueError(\n",
    "                    f\"Hidden state shape {last_hidden.shape} doesn't match expected \"\n",
    "                    f\"({self.num_layers}, {batch_size}, {self.hidden_size})\"\n",
    "                )\n",
    "\n",
    "        # RNN forward pass\n",
    "        rnn_out, hidden = self.rnn(x, last_hidden)\n",
    "\n",
    "        # Residual connection: project input and add to RNN output\n",
    "        residual = self.input_projection(x)\n",
    "        rnn_out_with_residual = rnn_out + residual\n",
    "\n",
    "        # Final output projection\n",
    "        out = self.fc(rnn_out_with_residual)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNetworkExplicit(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_rnn_layers):\n",
    "        super(RNNetworkExplicit, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        self.rnns = nn.ModuleList([nn.RNN(hidden_size, hidden_size, batch_first=True) for _ in range(num_rnn_layers)])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(num_rnn_layers)])\n",
    "        self.mlps = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        ) for _ in range(num_rnn_layers)])\n",
    "        self.lns2 = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(num_rnn_layers)])\n",
    "        self.output_proj = nn.Linear(hidden_size, output_size)\n",
    "        self.num_rnn_layers = num_rnn_layers  # Added to access in forward\n",
    "\n",
    "    def forward(self, x, hidden_state=None, return_hidden=False):\n",
    "        out = self.input_proj(x)\n",
    "        hidden_states = []\n",
    "        for i in range(self.num_rnn_layers):\n",
    "            residual = out\n",
    "            if hidden_state is not None:\n",
    "                h = hidden_state[i]\n",
    "            else:\n",
    "                h = None\n",
    "            out, h = self.rnns[i](out, h)\n",
    "            out = residual + out\n",
    "            out = self.lns[i](out)\n",
    "            residual = out\n",
    "            out = self.mlps[i](out)\n",
    "            out = residual + out\n",
    "            out = self.lns2[i](out)\n",
    "            hidden_states.append(h)\n",
    "        out = self.output_proj(out)\n",
    "        if return_hidden:\n",
    "            return out, hidden_states\n",
    "        return out\n"
   ],
   "id": "eb033544f464e4bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Test both implementations\n",
    "input_size = 10\n",
    "hidden_size = 768\n",
    "batch_size = 1024\n",
    "seq_len = 128\n",
    "learning_rate = 0.0005\n",
    "num_layers = 16\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0  # Gradient clipping value\n",
    "\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"smaLLM\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"RNN with Residuals\",\n",
    "        \"layer_size\": hidden_size,\n",
    "        \"seq_length\": seq_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dataset\": \"Wikipedia english\",\n",
    "        \"epochs\": epochs,\n",
    "        \"grad_clipping\": max_grad_norm,\n",
    "        \"mixed_precision\": True,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"LambdaLR with warmup\"\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Testing Sequential version:\")\n",
    "model1 = RNNetworkExplicit(128, 768, 128, num_rnn_layers=num_layers)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# add warm up to the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: min((step+1)/1000, 1))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for k, batch in enumerate(dataloader):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        targets = batch['target_ids'].to(device)\n",
    "        inputs_onehot = F.one_hot(inputs, num_classes=128).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision context\n",
    "        with autocast():\n",
    "            outputs = model1(inputs_onehot)\n",
    "            loss = criterion(outputs.view(-1, 128), targets.view(-1))\n",
    "\n",
    "        # Scale gradients, backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)  # Unscale before clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model1.parameters(), max_grad_norm)\n",
    "\n",
    "        # Optimizer step and scaler update\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        run.log({\"train/loss\": loss.item(),\n",
    "                  \"epoch\": epoch,\n",
    "                  \"batch\": k,\n",
    "                  \"lr\": scheduler.get_last_lr()[0]\n",
    "                 })\n",
    "\n",
    "        if k%1000 == 0 and k > 0:\n",
    "            torch.save(model1.state_dict(), f\"rnn/rnn_language_model_epoch{epoch}_batch{k}.pth\")\n"
   ],
   "id": "fcb28a704a9a74bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_sample(model, tokenizer, start_text, max_length):\n",
    "    model.eval()\n",
    "    generated = start_text\n",
    "    # Encode start_text without forcing full length\n",
    "    tokens = [ord(c) if ord(c) < tokenizer.vocab_size else tokenizer.unk_token for c in start_text]\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  # (1, len(tokens))\n",
    "    input_onehot = nn.functional.one_hot(input_ids, num_classes=tokenizer.vocab_size).float()\n",
    "    hidden = None\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_onehot, hidden, return_hidden=True)\n",
    "            next_token_logits = output[0, -1, :]  # (vocab_size)\n",
    "            # sample from the distribution\n",
    "            next_token_id = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1).item()\n",
    "            if next_token_id == tokenizer.pad_token:\n",
    "                break\n",
    "            generated += tokenizer.decode([next_token_id])\n",
    "            next_input_id = torch.tensor([[next_token_id]], dtype=torch.long)  # (1, 1)\n",
    "            input_onehot = nn.functional.one_hot(next_input_id, num_classes=tokenizer.vocab_size).float()\n",
    "    return generated\n"
   ],
   "id": "e7babaddb830acf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model1.to('cpu')\n",
    "out = generate_sample(model1, dataset.tokenizer, \"The meaning of life is\", max_length=100)\n",
    "print(out)"
   ],
   "id": "1209923df9728ba2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save the model\n",
    "torch.save(model1.state_dict(), \"rnn_final.pth\")"
   ],
   "id": "5aa6d1457759ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fbe6060267dd0c69",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
